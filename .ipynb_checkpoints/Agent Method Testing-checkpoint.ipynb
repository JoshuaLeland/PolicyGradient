{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import Agent\n",
    "from agent import sample_tracjectory, sample_trajectories, collect_trajectories\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_dict1 = {'ob_dim': 5, 'ac_dim': 2, 'discrete': True, 'size': 3, 'n_layers':5, 'learning_rate': 0.001}\n",
    "sample_traj1 = {'animate': False, 'max_path_length': 500, 'min_timesteps_per_batch': 10}\n",
    "return_args1 = {'gamma': 0.9, 'reward_to_go':True, 'nn_baseline':False, 'normalize_advantages': False}\n",
    "policy_dict2 = {'ob_dim': 5, 'ac_dim': 2, 'discrete': False, 'size': 3, 'n_layers':5, 'learning_rate': 0.001}\n",
    "return_args2 = {'gamma': 0.9, 'reward_to_go':False, 'nn_baseline':False, 'normalize_advantages': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build two agents for both test cases.\n",
    "a1 = Agent(policy_dict1, sample_traj1, return_args1)\n",
    "a2 = Agent(policy_dict2, sample_traj1, return_args2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Build their policy.\n",
    "a1.build_policy(False)\n",
    "a2.build_policy(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8248, -0.3088]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1.policy(torch.Tensor([[1, 2, 3, 4, 5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testTensor = torch.tensor([[1.0, 2.0, 0.0,4.,5.], [4., 5., 6.,1.,2.]])\n",
    "aa1 = a1.sample_action(a1.policy(testTensor))\n",
    "aa1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8281,  2.0324],\n",
       "        [-0.2729, -4.0973]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa2 = a2.sample_action(a2.policy(testTensor))\n",
    "aa2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4688, -0.4678], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1.get_log_prob(a1.policy(testTensor), aa1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2181, -3.0281],\n",
       "        [-0.4410, -3.9885]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2.get_log_prob(a2.policy(testTensor), aa2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3218, grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lossa1 = a1.calculate_loss(a1.policy(testTensor),aa1, torch.rand(2))\n",
    "lossa1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7605, -1.8904],\n",
      "        [-0.1744, -1.5767]], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.9348, 3.4671], grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lossa2 = a2.calculate_loss(a2.policy(testTensor),aa2, torch.rand(2))\n",
    "lossa2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.2666, 2.6019, 2.4120, 2.1421, 2.3564, 2.2504, 1.9629, 1.8291, 1.0181,\n",
       "        0.3334, 2.2680, 1.8112, 1.7684, 0.9666, 0.5263])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1.sum_of_rewards([torch.rand(10), torch.rand(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.2288, 3.2288, 3.2288, 3.2288, 3.2288, 3.2288, 3.2288, 3.2288, 3.2288,\n",
       "        3.2288, 1.9337, 1.9337, 1.9337, 1.9337, 1.9337])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2.sum_of_rewards([torch.rand(10), torch.rand(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.1989, 3.1989, 3.1989, 3.1989, 3.1989, 3.1989, 3.1989, 3.1989, 3.1989,\n",
       "         3.1989, 3.6417, 3.6417, 3.6417, 3.6417, 3.6417, 3.6417, 3.6417, 3.6417,\n",
       "         3.6417, 3.6417]),\n",
       " tensor([-0.9747, -0.9747, -0.9747, -0.9747, -0.9747, -0.9747, -0.9747, -0.9747,\n",
       "         -0.9747, -0.9747,  0.9747,  0.9747,  0.9747,  0.9747,  0.9747,  0.9747,\n",
       "          0.9747,  0.9747,  0.9747,  0.9747]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2.estimate_return(torch.rand(20,5),[torch.rand(10), torch.rand(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([4.0261, 3.4687, 2.8111, 2.8864, 2.7461, 2.2205, 1.7580, 1.1980, 0.9407,\n",
       "         0.3289]),\n",
       " tensor([4.0261, 3.4687, 2.8111, 2.8864, 2.7461, 2.2205, 1.7580, 1.1980, 0.9407,\n",
       "         0.3289]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1.estimate_return(torch.rand(10,5),[torch.rand(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Test trajectory helper functions.\n",
    "env = gym.make('Pendulum-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_dict3 = {'ob_dim': 3, 'ac_dim': 1, 'discrete': False, 'size': 3, 'n_layers':5, 'learning_rate': 0.001}\n",
    "sample_traj3 = {'animate': False, 'max_path_length': 500, 'min_timesteps_per_batch': 10}\n",
    "return_args3 = {'gamma': 0.9, 'reward_to_go':True, 'nn_baseline':True, 'normalize_advantages': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a3 = Agent(policy_dict3, sample_traj3, return_args3)\n",
    "a3.build_policy(double=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'observation': tensor([[0.6868, 0.7269, 3.4371]], dtype=torch.float64),\n",
       " 'reward': tensor([-1.2565], dtype=torch.float64),\n",
       " 'action': tensor([0.5511], dtype=torch.float64)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_tracjectory(a3, env, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths, num_steps = sample_trajectories(a3,1,env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_no, ac_na, re_n = collect_trajectories(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9903,  0.1392, -0.3735],\n",
       "        [-0.8773,  0.4799, -7.0720],\n",
       "        [ 0.0116,  0.9999,  1.2973],\n",
       "        [-0.0462,  0.9989, -5.9143],\n",
       "        [-0.9943, -0.1068, -0.3968],\n",
       "        [-0.7188,  0.6952, -0.4757],\n",
       "        [-0.8812, -0.4727,  5.3825],\n",
       "        [-0.6041,  0.7969, -1.8829],\n",
       "        [-0.9779,  0.2088, -2.1644],\n",
       "        [ 0.3342,  0.9425, -1.4021],\n",
       "        [-0.9788,  0.2047,  4.6834]], dtype=torch.float64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4912,  0.5614,  0.2058,  0.3304,  0.4951,  0.7038, -0.8254,  1.0492,\n",
       "         0.4469,  0.1929, -0.3591], dtype=torch.float64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ac_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([-9.1394], dtype=torch.float64),\n",
       " tensor([-14.2475], dtype=torch.float64),\n",
       " tensor([-2.2600], dtype=torch.float64),\n",
       " tensor([-8.1082], dtype=torch.float64),\n",
       " tensor([-9.1031], dtype=torch.float64),\n",
       " tensor([-5.8631], dtype=torch.float64),\n",
       " tensor([-11.7351], dtype=torch.float64),\n",
       " tensor([-6.0261], dtype=torch.float64),\n",
       " tensor([-9.7709], dtype=torch.float64),\n",
       " tensor([-2.1543], dtype=torch.float64),\n",
       " tensor([-9.2484], dtype=torch.float64)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = a3.sum_of_rewards(re_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_n, adv_n = a3.estimate_return(obs_no, re_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -9.1394, -14.2475,  -2.2600,  -8.1082,  -9.1031,  -5.8631, -11.7351,\n",
       "         -6.0261,  -9.7709,  -2.1543,  -9.2484], dtype=torch.float64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4399, -1.6663,  2.0263, -1.0364,  0.2232, -0.1147,  0.6248, -0.2411,\n",
       "        -0.6072,  0.2904,  0.9409], dtype=torch.float64,\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_hist = a3.policy(obs_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp = a3.get_log_prob(param_hist, ac_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4399, -1.6663,  2.0263, -1.0364,  0.2232, -0.1147,  0.6248, -0.2411,\n",
       "        -0.6072,  0.2904,  0.9409], dtype=torch.float64,\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
